# -*- coding: utf-8 -*-
"""DL Practical Quesiton Answering Using LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NOJCqjQSi1muxSHGH-OSkPTKyahos3TT
"""

# import numpy as np

# from nltk.tokenize import word_tokenize

# import tensorflow as tf
# from tensorflow.keras.layers import Input
# from keras.layers.core import Activation, Dense, Dropout, Permute
# from tensorflow.keras.layers.embeddings import Embedding
# from tensorflow.keras.layers.merge import add, concatenate, dot
# from tensorflow.keras.layers.recurrent import LSTM
# from tensorflow.keras.models import Model, Sequential
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# from tensorflow.keras.utils import np_utils
# from tensorflow.keras.utils import get_file
# from tensorflow.keras.utils import plot_model
# import matplotlib.pyplot as plt

# import tarfile
# import re
# from functools import reduce
# import nltk
# nltk.download('punkt')
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

import numpy as np
from nltk.tokenize import word_tokenize

import tensorflow as tf
from tensorflow.keras.layers import Input, Activation, Dense, Dropout, Permute, Embedding, add, concatenate, dot, LSTM
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical, get_file, plot_model
import matplotlib.pyplot as plt

import tarfile
import re
from functools import reduce


# import numpy as np
# from nltk.tokenize import word_tokenize

# import tensorflow as tf
# from tensorflow.keras.layers import Input, Activation, Dense, Dropout, Permute, Embedding, add, concatenate, dot, LSTM
# from tensorflow.keras.models import Model, Sequential
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# from tensorflow.keras.utils import to_categorical, plot_model
# import matplotlib.pyplot as plt

# import tarfile
# import re
# from functools import reduce

from google.colab import drive
drive.mount('/content/drive')

import tarfile
path = '/content/drive/MyDrive/deepLearningDataset/babi_tasks_1-20_v1-2.tar.gz'
tar = tarfile.open(path)

# def tokenize(sent):
#     '''
#     argument: a sentence string
#     returns a list of tokens(words)
#     '''
#     return word_tokenize(sent)

def tokenize(sent):
    '''
    argument: a sentence string
    returns a list of tokens(words)
    '''
    return word_tokenize(sent)

def parse_stories(lines):
    '''
    - Parse stories provided in the bAbI tasks format
    - A story starts from line 1 to line 15. Every 3rd line,
      there is a question &amp;amp;amp;amp;amp; answer.
    - Function extracts sub-stories within a story and
      creates tuples
    '''
    data = []
    story = []
    for line in lines:
        line = line.decode('utf-8').strip()
        nid, line = line.split(' ', 1)
        nid = int(nid)
        if nid == 1:
            # reset story when line ID=1 (start of new story)
            story = []
        if '\t' in line:
            # this line is tab separated Q, A &amp;amp;amp;amp;amp; support fact ID
            q, a, supporting = line.split('\t')
            # tokenize the words of question
            q = tokenize(q)
            # Provide all the sub-stories till this question
            substory = [x for x in story if x]
            # A story ends and is appended to global story data-set
            data.append((substory, q, a))
            story.append('')
        else:
            # this line is a sentence of story
            sent = tokenize(line)
            story.append(sent)
    return data

def get_stories(f):
    '''
    argument: filename
    returns list of all stories in the argument data-set file
    '''
    # read the data file and parse 10k stories
    data = parse_stories(f.readlines())
    # lambda func to flatten the list of sentences into one list
    flatten = lambda data: reduce(lambda x, y: x + y, data)
    # creating list of tuples for each story
    data = [(flatten(story), q, answer) for story, q, answer in data]
    return data

challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'
print('Extracting stories')
# Extracting train stories
train_stories = get_stories(tar.extractfile(challenge.format('train')))
# Extracting test stories
test_stories = get_stories(tar.extractfile(challenge.format('test')))

vocab = set()
for story,question,answer in train_stories + test_stories:
    vocab |= set(story + question + [answer])
vocab = sorted(vocab)
vocab_size = len(vocab) +1
print(f"Lenght of Vocab  : {len(vocab)}")

word_to_idx = {w:idx for idx,w in enumerate(vocab)}
idx_to_word = {idx:w for idx,w in enumerate(vocab)}

story_maxlen = max(map(len,(x for x,_,_ in train_stories +test_stories)))
query_maxlen = max(map(len,(x for _,x,_ in train_stories +test_stories)))

def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):
    # story vector initialization
    X = []
    # query vector initialization
    Xq = []
    # answer vector intialization
    Y = []
    for story, query, answer in data:
        # creating list of story word indices
        x = [word_idx[w] for w in story]
        # creating list of query word indices
        xq = [word_idx[w] for w in query]
        # let's not forget that index 0 is reserved
        y = np.zeros(len(word_idx) + 1)
        # creating label 1 for the answer word index
        y[word_idx[answer]] = 1
        X.append(x)
        Xq.append(xq)
        Y.append(y)
    return (pad_sequences(X, maxlen=story_maxlen),
            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))

# vectorize train story, query and answer sentences using vocab
inputs_train, queries_train, answers_train = vectorize_stories(train_stories,word_to_idx,story_maxlen,query_maxlen)
# vectorize test story, query and answer sentences vocab
inputs_test, queries_test, answers_test = vectorize_stories(test_stories,word_to_idx,story_maxlen,query_maxlen)

train_epochs = 50
batch_size = 32
embed_size = 50
lstm_size = 32
dropout_rate = 0.30

# Pad the question sequences to match the story sequence length
queries_train_padded = pad_sequences(queries_train, maxlen=story_maxlen, padding='post', truncating='post')
queries_test_padded = pad_sequences(queries_test, maxlen=story_maxlen, padding='post', truncating='post')

# Define two inputs: one for the story and padded question input
input_sequence = Input(shape=(story_maxlen,))
question = Input(shape=(story_maxlen,))

# Embedding layer for the story sequence
input_encoder = Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=story_maxlen)(input_sequence)

# Embedding layer for the padded question sequence
question_encoder = Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=story_maxlen)(question)

# Combine story and question embeddings (concatenate)
merged = concatenate([input_encoder, question_encoder])

# LSTM layer to process the combined sequences
lstm_out = LSTM(lstm_size, dropout=dropout_rate, recurrent_dropout=dropout_rate)(merged)

# Dense layer for output (softmax for classification)
output = Dense(vocab_size, activation='softmax')(lstm_out)

# Create the model
model = Model(inputs=[input_sequence, question], outputs=output)

# Compile the model
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
# print(model.summary())

# Start training the model
history = model.fit([inputs_train, queries_train_padded], answers_train,
                    batch_size=batch_size,
                    epochs=train_epochs,
                    validation_data=([inputs_test, queries_test_padded], answers_test))

# Save the trained model
model.save('model.h5')

input_sequence = Input((story_maxlen,))
question = Input((query_maxlen,))

print('Input sequence:', input_sequence)
print('Question:', question)

input_encoder_m = Sequential()
input_encoder_m.add(Embedding(input_dim=vocab_size,
                              output_dim=embed_size))
input_encoder_m.add(Dropout(dropout_rate))

# embed the input into a sequence of vectors of size query_maxlen
input_encoder_c = Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_size,
                              output_dim=query_maxlen))
input_encoder_c.add(Dropout(dropout_rate))
# output: (samples, story_maxlen, query_maxlen)


question_encoder = Sequential()
question_encoder.add(Embedding(input_dim=vocab_size,
                               output_dim=embed_size,
                               input_length=query_maxlen))
question_encoder.add(Dropout(dropout_rate))
# output: (samples, query_maxlen, embedding_dim)


input_encoded_m = input_encoder_m(input_sequence)
print('Input encoded m', input_encoded_m)
input_encoded_c = input_encoder_c(input_sequence)
print('Input encoded c', input_encoded_c)
question_encoded = question_encoder(question)
print('Question encoded', question_encoded)

# compute a 'match' between the first input vector sequence

match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)
print(match.shape)
match = Activation('softmax')(match)
print('Match shape', match)

# add the match matrix with the second input vector sequence
response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)
response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)
print('Response shape', response)

# concatenate the response vector with the question vector sequence
answer = concatenate([response, question_encoded])
print('Answer shape', answer)

answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32
answer = Dropout(dropout_rate)(answer)
answer = Dense(vocab_size)(answer)  # (samples, vocab_size)
# we output a probability distribution over the vocabulary
answer = Activation('softmax')(answer)

# build the final model
model = Model([input_sequence, question], answer)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy',
              metrics=['accuracy'])

print(model.summary())

# start training the model
history = model.fit([inputs_train, queries_train], answers_train,
                    batch_size=batch_size,
                    epochs=train_epochs,
                    validation_data=([inputs_test, queries_test], answers_test))

# save model
model.save('model.h5')

for i in range(0,10):
    current_inp = test_stories[i]
    current_story, current_query, current_answer = vectorize_stories([current_inp], word_to_idx, story_maxlen, query_maxlen)
    current_prediction = model.predict([current_story, current_query])
    current_pred = idx_to_word[np.argmax(current_prediction)]
    ques = " ".join(current_inp[1])
    print(f"Question : {ques} \nAnswer : {current_pred}")
    print("--------------------------------\t")

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel("loss")
plt.xlabel('epoch')
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel("accuracy")
plt.xlabel('epoch')
plt.show()

